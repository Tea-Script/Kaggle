{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "\n",
    "Our task is to forecast the total amount of products sold in every shop for the test set.\n",
    "Model should handle shops and products changing each month\n",
    "\n",
    "\n",
    " ## Files\n",
    "\n",
    "Training Set: `sales_train.csv`\n",
    "\n",
    "Test set: `test.csv`: Forecast sales for these shops \n",
    "\n",
    "`sample_submission.csv`: the expected format of a submission\n",
    "\n",
    "`items.csv`: supplemental info about items/products\n",
    "\n",
    "`item_categories.csv`: supplemental info about item categories\n",
    "\n",
    "`shops.csv`: supplemental info about shops\n",
    "\n",
    " ## Variables \n",
    "\n",
    "`ID`: represents a shop item within the test set\n",
    "\n",
    "`shop_id`: unique identifier of a shop\n",
    "\n",
    "`item_id`: unique identifier of a product\n",
    "\n",
    "`item_category_id`: unique identifier of an item category\n",
    "\n",
    "`item_cnt_day`: number of products sold in a day\n",
    "\n",
    "`item_price`: current price of item\n",
    "\n",
    "`date`: dd/mm/yyyy\n",
    "\n",
    "`date_Block_num`: January 2013 is 0. Increment every month until October 2015 (33)\n",
    "\n",
    "`item_name`: name of item\n",
    "\n",
    "`shop_name`: name of shop\n",
    "\n",
    "`item_category_name`: name of item category\n",
    "\n",
    "\n",
    "We are going to ignore data such as names of items, shops and item categories, as we will not be using text analysis to look for trends in sales based on name similarity, only on the items, shops and categories themselves. As such, ids are more useful. Additionally, `DATE` will be transformed into more useful data that ignores years.\n",
    "\n",
    "\n",
    "The input to the model should be the variables `ID`, `shop_id`, and `item_id`, therefore, we must train a model that can recognize monthly trends in sales per shop per item to predict next month's data from this month's. \n",
    "\n",
    "Our model also knows that the current `date_block_num` is 34 because we are predicting for just November 2015, so we can use `date`, and `date_block_num` as part of the model input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "It is necessary that we combine `item_cnt_day` into `item_cnt_month` for each item in each shop and graph to see what kind of trends each item in each shop have over the past two years. We can overlay the data for each item with everyshop that carried it that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month  month_count\n",
      "0     0      0           16\n",
      "1     1      1           10\n",
      "2     2      2            7\n",
      "3     2      3            5\n",
      "4     3      3            5\n",
      "5     3      4            4\n",
      "2.86 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2 ms ± 90.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "DATA_DIR = \"~/.kaggle/competitions/competitive-data-science-final-project/\"\n",
    "\n",
    "# TODO:\n",
    "# read in csv file, remove headers and date, sort by date_block_num by item_id by store_id with custom sort criteria\n",
    "# aggregate variable item_cnt_day in last column for each store in date_block_num as new row item_cnt_month\n",
    "train_set = pd.read_csv(DATA_DIR + \"sales_train.csv\")  \n",
    "train_set=train_set.iloc[: , 1:]\n",
    "train_set = train_set.sort_values(by=[\"date_block_num\", \"shop_id\",\"item_id\"])\n",
    "#train_set = train_set.agg([\"sum\"])\n",
    "#print(train_set)\n",
    "\n",
    "\n",
    "test = pd.DataFrame({\"year\": [0,0,0,0,1,1,1,2,2,2,2,3,3,3,3], \"month\" : [0,0,0,0,1,1,1,2,2,2,3,3,3,4,4], \"day\" : [0,0,0,1,1,1,2,2,2,2,3,3,4,4,5], \"day_count\" : [7,4,3,2,1,5,4,2,3,2,5,3,2,1,3]})\n",
    "test = test[[\"year\", \"month\", \"day\", \"day_count\"]]\n",
    "#print(test)\n",
    "\n",
    "def agg_multiple(df, labels, aggvar, repl=None):\n",
    "    if(repl is None): repl = aggvar\n",
    "    conds = df.duplicated(labels).tolist() #returns boolean list of false for a unique (year,month) then true until next unique pair\n",
    "    groups = []\n",
    "    start = 0\n",
    "    for i in range(len(conds)): #When false, split previous to new df, aggregate count \n",
    "        bul = conds[i]\n",
    "        if(i == len(conds) - 1): i +=1 #no false marking end of last group, special case\n",
    "        if not bul and i > 0 or bul and i == len(conds): \n",
    "            sample = df.iloc[start:i , :]\n",
    "            start = i\n",
    "            sample = sample.groupby(labels, as_index=False).agg({aggvar:sum}).rename(columns={aggvar : repl})\n",
    "            groups.append(sample)\n",
    "    df = pd.concat(groups).reset_index(drop=True) #combine aggregated dfs into new df\n",
    "    return df\n",
    "test2 = agg_multiple(test, [\"year\", \"month\"], \"day_count\", repl=\"month_count\")\n",
    "print(test2)\n",
    "%timeit test.groupby([\"year\", \"month\"], as_index=False).agg({\"day_count\":sum}).rename(columns={\"day_count\":\"month_count\"})\n",
    "\n",
    "%timeit test.groupby(['year', 'month']).day_count.sum().to_frame('month_count').reset_index()\n",
    "\n",
    "\n",
    "def plotrandom(train_set, items, stores):\n",
    "    for i in range(stores):\n",
    "        store = np.random.choice(train_set[\"shop_id\"])\n",
    "        for j in range(items):\n",
    "            \n",
    "            train_set2 = train_set.loc[train_set['shop_id'] == store]        \n",
    "            item = random.choice(train_set['item_id'])\n",
    "            train_set2 = train_set.loc[train_set['item_id'] == item]\n",
    "\n",
    "\n",
    "            train_set2 = agg_multiple(train_set2, [\"date_block_num\", \"shop_id\", \"item_id\"], \"item_cnt_day\", repl=\"item_cnt_mnth\")\n",
    "\n",
    "            #TODO: Graph month by item_cnt_month for distinct item_id, and draw a seperate line for 5 chosen stores\n",
    "            #####  Repeat for 5 items\n",
    "            #####  Analyze graphs\n",
    "            x = train_set2[\"date_block_num\"]\n",
    "            y = train_set2[\"item_cnt_mnth\"]\n",
    "            plt.plot(x,y)\n",
    "        plt.show()\n",
    "#plotrandom(train_set, 2,2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date_block_num  shop_id  item_id  item_cnt_mnth\n",
      "0                   2       47    12472            1.0\n",
      "1                   9       25    14829            6.0\n",
      "2                  11       25    11616            1.0\n",
      "3                   8       27     9659            1.0\n",
      "4                  24       48     6472            1.0\n",
      "5                  16       57     8415            2.0\n",
      "6                  18       24     6316            1.0\n",
      "7                  23       25    16203            1.0\n",
      "8                   0       41    11041            2.0\n",
      "9                   8        6    16832            1.0\n",
      "10                 23       15     3341            3.0\n",
      "11                  3        3    15369            1.0\n",
      "12                 23       31     1556            1.0\n",
      "13                  9       55     9524            1.0\n",
      "14                  8       25     7098            2.0\n",
      "15                  0       31    13605            1.0\n",
      "16                  0       41    19164            1.0\n",
      "17                 22       17    12814            1.0\n",
      "18                 12       16    15023            1.0\n",
      "19                  2       54    13156            1.0\n",
      "20                  8       53     7047            1.0\n",
      "21                 32       46     2819            1.0\n",
      "22                 30        7    13440            1.0\n",
      "23                 32        2    16170            1.0\n",
      "24                 22        2     3076            1.0\n",
      "25                 23       31    17595            1.0\n",
      "26                 29       47    18043            1.0\n",
      "27                 28       56    10021            1.0\n",
      "28                 12       15    12970            1.0\n",
      "29                  1       27    18603            1.0\n",
      "...               ...      ...      ...            ...\n",
      "29328               8       27     7055            1.0\n",
      "29329              13       57     2754            1.0\n",
      "29330               0       47     6777            1.0\n",
      "29331               0       51    12461            1.0\n",
      "29332              21       58    18481            1.0\n",
      "29333              14       58     7894            1.0\n",
      "29334              11       15    15064            1.0\n",
      "29335               8       52    15537            1.0\n",
      "29336              18       12     4872            4.0\n",
      "29337               8       47     3871            1.0\n",
      "29338              15       31    15091            1.0\n",
      "29339               2        6    10474            1.0\n",
      "29340               1       35    18634            1.0\n",
      "29341              29       42     3329            2.0\n",
      "29342              27       21    10515            1.0\n",
      "29343              11       15     2832            1.0\n",
      "29344              13        4    20949            2.0\n",
      "29345              11        5     3314            1.0\n",
      "29346               0       25    21861            1.0\n",
      "29347              22       37     2294            1.0\n",
      "29348              10       54    14888            1.0\n",
      "29349              11       59     7203            2.0\n",
      "29350               4       56    20949            2.0\n",
      "29351              18       54    12860            1.0\n",
      "29352              15       53     2833            1.0\n",
      "29353              19       21     6086            1.0\n",
      "29354              19       52    10651            1.0\n",
      "29355              28       22    12730            1.0\n",
      "29356              15        6    16184            1.0\n",
      "29357              31       21     5669            1.0\n",
      "\n",
      "[29358 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = agg_multiple(train_set.sample(frac=.01), [\"date_block_num\", \"shop_id\", \"item_id\"], \"item_cnt_day\", repl=\"item_cnt_mnth\")\n",
    "dataset[dataset[\"item_cnt_mnth\"] > 20] = 20\n",
    "print(dataset)\n",
    "train_set = dataset.sample(frac=.75)\n",
    "test_set =  dataset.sample(frac=.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no obvious pattern in the graphs for any item along stores, or stores along items. We assume the two are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "For this problem we will be using polynomial regression, random forests, and an SVM, optimizing each, and then comparing the results on the validation set, which will be a small split from the training set. If all perform well, a voting system will be installed and all three will be responsible for determining each prediction for each store and item as a rudimentary ensemble method.\n",
    "\n",
    "\n",
    "The file `tests.csv` contains the set of all training data. We must read that in, and create a prediction for each `ID`. that `ID` should then be marked in the new submission csv before the prediction. `ID` is redundant with `shop_id` and `item_id`, so we will not train with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "Polynomial regression isn't too difficult to model. Using the library scikit learn, we simply need to seperate the expected from the training matrix and run a few functions. We can then make predictions based on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.80000000e+01 2.60000000e+01 ... 3.65599322e+29\n",
      "  3.08790812e+31 2.60809471e+33]\n",
      " [1.00000000e+00 2.00000000e+00 3.00000000e+01 ... 1.15564191e+34\n",
      "  2.98040049e+36 7.68645287e+38]\n",
      " [1.00000000e+00 2.30000000e+01 3.00000000e+01 ... 5.57390580e+27\n",
      "  2.33360856e+29 9.77004117e+30]\n",
      " ...\n",
      " [1.00000000e+00 2.50000000e+01 5.20000000e+01 ... 5.55412280e+31\n",
      "  3.69562786e+33 2.45901392e+35]\n",
      " [1.00000000e+00 1.40000000e+01 4.10000000e+01 ... 1.49393133e+36\n",
      "  4.78786773e+38 1.53445322e+41]\n",
      " [1.00000000e+00 1.30000000e+01 2.70000000e+01 ... 2.53304596e+31\n",
      "  3.46652031e+33 4.74399724e+35]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "-0.003803313929410024\n",
      "true accuracy is  0.0\n",
      "true accuracy is  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(train_set[[\"date_block_num\", \"shop_id\",\"item_id\"]])\n",
    "X_test =  np.array(test_set[[\"date_block_num\", \"shop_id\",\"item_id\"]])\n",
    "y = np.array(train_set[[\"item_cnt_mnth\"]])\n",
    "y_test = np.array(test_set[[\"item_cnt_mnth\"]])\n",
    "# PolynomialFeatures (prepreprocessing)\n",
    "poly = PolynomialFeatures(degree=10, include_bias=True)\n",
    "X_ = poly.fit_transform(X)\n",
    "X_test_ = poly.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# Instantiate\n",
    "lg = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "lg.fit(X_, y)\n",
    "\n",
    "# Obtain coefficients\n",
    "lg.coef_\n",
    "\n",
    "\n",
    "\n",
    "print(X_)\n",
    "preds = np.round(lg.predict(X_test_))\n",
    "print(preds)\n",
    "print(lg.score(X_test_, y_test))\n",
    "#accuracy function\n",
    "# find absolute difference between predictions and values\n",
    "# add differences together\n",
    "# take 1 - number of differences/total sum\n",
    "\n",
    "\n",
    "#true accuracy function\n",
    "# add up total correct, divide by total\n",
    "true = np.sum((preds == y_test.ravel )) / len(y_test) \n",
    "print(\"true accuracy is \", true)\n",
    "\n",
    "preds = np.round(lg.predict(X_))\n",
    "true = np.sum((preds == y.ravel )) / len(y) \n",
    "print(\"true accuracy is \", true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is clearly unsuccessful, with a correlation coefficient of close to 0. It achieves a prediction accuracy of 20% through shear overfitting. The data is clearly not well-mapped by a polynomial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "To model the data as a random forest we can again use the sklearn random forest regression in the ensemble library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8079589865289066\n",
      "true accuracy is  0.9182561307901907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#np.nan_to_num(X_test, copy=0)\n",
    "\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators = 50, max_depth=None, random_state=37, oob_score=1)\n",
    "regr.fit(X, y.ravel())\n",
    "preds = np.round(regr.predict(X_test))\n",
    "print(regr.score(X_test, y_test.ravel()))\n",
    "#accuracy function\n",
    "# find absolute difference between predictions and values\n",
    "# add differences together\n",
    "# take 1 - number of differences/total sum\n",
    "pred = np.round(regr.predict(X))\n",
    "\n",
    "#true accuracy function\n",
    "# add up total correct, divide by total\n",
    "true = np.sum((preds == y_test.ravel() )) / len(y_test) \n",
    "\n",
    "print(\"true accuracy is \", true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are not good for problems where the nodes will encounter features they've never seen, that is why the test set only has 37% success, no matter how we try to retune the parameters. This dataset requires a more flexible model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5734857195981953\n",
      "0.6049434148817315\n",
      "true accuracy is  0.9182561307901907\n",
      "Hyper Parameter Tuning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#parameters = {\\'kernel\\':(\\'sigmoid\\', \\'poly\\'), \\'C\\':[.1, 10, 1000], \\'epsilon\\':[.001, .1], \\'gamma\\' : [.001, 1, 1000], \\'degree\\' : [3, 7]}\\nPar1 = {\\'kernel\\' : [\\'sigmoid\\', \\'rbf\\'], \\'C\\': [.001, .1, 100, 1000], \\'epsilon\\':[.1], \\'gamma\\': [.0001, .001, .01, .1]}\\n\\nclf1 = GridSearchCV(regr, Par1)\\nclf1.fit(X, y.ravel())\\n\\nprint(clf1.score(X_test, y_test.ravel()))\\nprint(clf1.best_estimator_)\\n\\npreds = np.round(clf1.predict(X_test))\\ntrue = np.sum((preds == y_test.ravel() )) / len(y_test) \\nprint(\"true accuracy is \", true)\\n\\n#Par2 = {\\'kernel\\' : [\\'poly\\'], \\'C\\': [.1, 1], \\'epsilon\\':[.1], \\'gamma\\': [.1, 10], \\'degree\\':[5]}\\n\\n#clf2 = GridSearchCV(regr, Par2)\\n#clf2.fit(X, y.ravel())\\n\\n#print(clf2.score(X_test, y_test.ravel()))\\n#print(clf2.best_estimator_)\\n\\n#preds = np.round(clf2.predict(X_test))\\n#true = np.sum((preds == y_test.ravel() )) / len(y_test) \\n#print(\"true accuracy is \", true)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#postval_set = agg_multiple(dataset, [\"date_block_num\", \"shop_id\", \"item_id\"], \"item_cnt_day\", repl=\"item_cnt_mnth\")\n",
    "#X_ = np.array(postval_set[[\"date_block_num\", \"shop_id\",\"item_id\"]])\n",
    "#y_ = np.array(postval_set[[\"item_cnt_mnth\"]])\n",
    "\n",
    "#regr = SVR(C=10, epsilon = .1, gamma=.01, kernel=\"sigmoid\")\n",
    "regr = SVR(C=10, epsilon = .1, gamma=.01, kernel=\"rbf\")\n",
    "\n",
    "#regr = SVR(C=10, epsilon = .1, gamma=.01, kernel=\"poly\", degree=15)\n",
    "regr.fit(X, y.ravel())\n",
    "preds = np.round(regr.predict(X_test))\n",
    "print(regr.score(X_test, y_test.ravel()))\n",
    "print(regr.score(X, y.ravel()))\n",
    "\n",
    "#true accuracy function\n",
    "# add up total correct, divide by total\n",
    "#true = np.sum((preds == y_test.ravel() )) / len(y_test) \n",
    "\n",
    "print(\"true accuracy is \", true)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Hyper Parameter Tuning\")\n",
    "'''\n",
    "#parameters = {'kernel':('sigmoid', 'poly'), 'C':[.1, 10, 1000], 'epsilon':[.001, .1], 'gamma' : [.001, 1, 1000], 'degree' : [3, 7]}\n",
    "Par1 = {'kernel' : ['sigmoid', 'rbf'], 'C': [.001, .1, 100, 1000], 'epsilon':[.1], 'gamma': [.0001, .001, .01, .1]}\n",
    "\n",
    "clf1 = GridSearchCV(regr, Par1)\n",
    "clf1.fit(X, y.ravel())\n",
    "\n",
    "print(clf1.score(X_test, y_test.ravel()))\n",
    "print(clf1.best_estimator_)\n",
    "\n",
    "preds = np.round(clf1.predict(X_test))\n",
    "true = np.sum((preds == y_test.ravel() )) / len(y_test) \n",
    "print(\"true accuracy is \", true)\n",
    "\n",
    "#Par2 = {'kernel' : ['poly'], 'C': [.1, 1], 'epsilon':[.1], 'gamma': [.1, 10], 'degree':[5]}\n",
    "\n",
    "#clf2 = GridSearchCV(regr, Par2)\n",
    "#clf2.fit(X, y.ravel())\n",
    "\n",
    "#print(clf2.score(X_test, y_test.ravel()))\n",
    "#print(clf2.best_estimator_)\n",
    "\n",
    "#preds = np.round(clf2.predict(X_test))\n",
    "#true = np.sum((preds == y_test.ravel() )) / len(y_test) \n",
    "#print(\"true accuracy is \", true)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR with a sigmoid kernel is the best so far with 90% accuracy with params C = 1, gamma = .001\n",
    "\n",
    "SVR with rbf kernel 90% accuracy with params C=1000, gamma = .0001\n",
    "\n",
    "SVR with polynomial kernel won't run\n",
    "\n",
    "Based on the results, SVR with a sigmoid kernel will perform the best on the dataset, but given the higher score for rbf, rbf fits the data better. both will be used in the next stages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
